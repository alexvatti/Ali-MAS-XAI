{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Web Service Classification using BERT (RoBERTa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Load Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "# Text Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from transformers import (\n",
    "    RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    ")\n",
    "import warnings\n",
    "import torch\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "os.makedirs(\"charts\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior (optional: slower)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Report in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_csv(report_dict, confusion_mat, class_labels, class_names, output_file=\"BERT-Report.csv\"):\n",
    "    df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "    # Filter only the class rows (not avg/macro)\n",
    "    filtered_labels = [str(class_names[i]) for i in class_labels]\n",
    "    df = df.loc[filtered_labels].copy()\n",
    "\n",
    "    # Compute per-class accuracy\n",
    "    row_sums = confusion_mat.sum(axis=1)\n",
    "    diagonal = np.diag(confusion_mat)\n",
    "    per_class_accuracy = (diagonal / row_sums).round(2)\n",
    "\n",
    "    df['accuracy'] = per_class_accuracy\n",
    "    if 'support' in df.columns:\n",
    "        df = df.drop(columns=['support'])\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'Category'}, inplace=True)\n",
    "\n",
    "    output_path = f\"results/{output_file}\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Classification report saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model -Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, average='weighted', zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "    }\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def plot_confusion_and_report(y_true, y_pred, class_names, n):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    figsize = (18, 18) if n >= 40 else (8, 8)\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"Confusion Matrix - Top {n} Categories\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"charts/BERT_confusion_matrix_top_{n}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"classification_report for {n} Web Serivce Categories\")\n",
    "    # Convert class_names to list of strings\n",
    "    class_names_str = [str(name) for name in class_names]\n",
    "    \n",
    "    # Get sorted list of unique label integers\n",
    "    labels = sorted(np.unique(y_true))\n",
    "\n",
    "    # classification_report for print\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names_str, zero_division=0))\n",
    "\n",
    "    # classification_report for dictionary\n",
    "    report_dict = classification_report(y_true, y_pred, target_names=class_names_str, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    # Save to CSV\n",
    "    generate_report_csv(report_dict, cm, class_labels=labels, class_names=class_names_str, output_file=f\"BERT-Report-Top-{n}.csv\")\n",
    "\n",
    "    return report_dict\n",
    "\n",
    "def plot_metrics(bert_results):\n",
    "    categories = list(bert_results.keys())\n",
    "    accuracies = [bert_results[n]['accuracy'] for n in categories]\n",
    "    precisions = [bert_results[n]['precision'] for n in categories]\n",
    "    recalls = [bert_results[n]['recall'] for n in categories]\n",
    "    f1_scores = [bert_results[n]['f1_score'] for n in categories]\n",
    "\n",
    "    def plot_metric(x, y, ylabel, title, color='blue'):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(x, y, marker='o', linestyle='-', color=color)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Number of Web Serivce Categories')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.grid(True)\n",
    "        plt.xticks(x)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"charts/{title}.png\")\n",
    "        plt.show()\n",
    "\n",
    "    plot_metric(categories, accuracies, 'Accuracy', 'BERT Model Performace Accuracy', color='green')\n",
    "    plot_metric(categories, precisions, 'Precision', 'BERT Model Performace Precision', color='orange')\n",
    "    plot_metric(categories, recalls, 'Recall', 'BERT Model Performace Recall', color='purple')\n",
    "    plot_metric(categories, f1_scores, 'F1 Score', 'BERT Model Performace F1 Score', color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
    "\n",
    "    return \" \".join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(n):\n",
    "    df = pd.read_csv(f\"../data/Balanced_Top_{n}_Web_Services.csv\").dropna()\n",
    "    df = df[['Service Description', 'Grouped Category']].rename(columns={'Service Description': 'text', 'Grouped Category': 'label'})\n",
    "\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['label'])\n",
    "    num_labels = df['label'].nunique()\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    dataset = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "    dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_labels)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_top_{n}\",\n",
    "        num_train_epochs=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=f\"./logs_top_{n}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        seed=42,  \n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['test'],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    preds = trainer.predict(tokenized_dataset['test'])\n",
    "    y_true = preds.label_ids\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    metrics = compute_metrics((preds.predictions, y_true))\n",
    "\n",
    "    report = plot_confusion_and_report(y_true, y_pred, class_names, n)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"f1_score\": metrics[\"f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run and collect results\n",
    "bert_results = []\n",
    "for n in [50]:\n",
    "    print(f\"\\n Processing Top {n} Web Service Categories\")\n",
    "    bert_results.append(train_and_evaluate(n))\n",
    "\n",
    "# Save results\n",
    "with open(\"results/bert_roberta_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bert_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "print(bert_results)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot settings\n",
    "for metric_dict in bert_results:\n",
    "    model_name = metric_dict['model']\n",
    "    metrics = {k: v for k, v in metric_dict.items() if k != 'model'}\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(metrics.keys(), metrics.values())\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0.7, 0.8)  # Adjust range as needed\n",
    "    plt.title(f'{model_name} Performance Metrics')\n",
    "\n",
    "    # Annotate bar values\n",
    "    for key, value in metrics.items():\n",
    "        plt.text(key, value + 0.002, f\"{value:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
