{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6d35b2",
   "metadata": {
    "cellUniqueIdByVincent": "a239b"
   },
   "source": [
    "# Objective : Web Services  Classification \n",
    "## Our goal is to build a baseline model with at least 80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911437a",
   "metadata": {
    "cellUniqueIdByVincent": "7d070"
   },
   "source": [
    "## 1. Load Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21369177",
   "metadata": {
    "cellUniqueIdByVincent": "ea649"
   },
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Encoding and Scaling\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Model Selection & Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tabulate import tabulate\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup directories\n",
    "os.makedirs(\"charts\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7580a",
   "metadata": {
    "cellUniqueIdByVincent": "49251"
   },
   "source": [
    "## 2. Reading the Web Services dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa658f",
   "metadata": {
    "cellUniqueIdByVincent": "ad086"
   },
   "outputs": [],
   "source": [
    "input_csv = f\"../data/Balanced_Top_50_Web_Services.csv\"\n",
    "df = pd.read_csv(input_csv)\n",
    "print(df)\n",
    "print(df[\"Grouped Category\"].nunique())\n",
    "print(df[\"Grouped Category\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda68012",
   "metadata": {
    "cellUniqueIdByVincent": "78587"
   },
   "source": [
    "## 3. Basic Inspection on given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e59cdb",
   "metadata": {
    "cellUniqueIdByVincent": "8f7b3"
   },
   "outputs": [],
   "source": [
    "def basic_inspection_dataset(table):\n",
    "    print(\"Top 5 Records of dataset\")\n",
    "    print(table.head())\n",
    "    print()\n",
    "        \n",
    "    print(\"Bottom 5 Records of dataset\")\n",
    "    print(table.tail())\n",
    "    print()\n",
    "    \n",
    "    print(\"Column/features/Variable  - Names of Given dataset\")\n",
    "    print(table.columns)\n",
    "    print()\n",
    "    \n",
    "    print(\"Shape(rows x columns) - of Given dataset\")\n",
    "    print(table.shape)\n",
    "    print()\n",
    "    \n",
    "    print(\"Data types - Given Column Names\")\n",
    "    print(table.dtypes)\n",
    "    print()\n",
    "    \n",
    "    print(\"Summry of dataset\")\n",
    "    print(table.info())\n",
    "    print()\n",
    "    \n",
    "    print(\"To see the count of null/nan values in columns of dataset\")\n",
    "    print(table.isnull().value_counts())\n",
    "    print()\n",
    "    \n",
    "    print(\"Dataset Summary \")\n",
    "    print(table.describe())\n",
    "    print()\n",
    "    \n",
    "basic_inspection_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7a7e8",
   "metadata": {
    "cellUniqueIdByVincent": "ab4fe"
   },
   "source": [
    "## 4. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6968105b",
   "metadata": {
    "cellUniqueIdByVincent": "284f1"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3770b36",
   "metadata": {
    "cellUniqueIdByVincent": "bebd0"
   },
   "source": [
    "## 5. Categorical- UniVariable - Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32187c3",
   "metadata": {
    "cellUniqueIdByVincent": "e9b08",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class BarPieChartTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df=X.copy()\n",
    "        # get cat columns \n",
    "        cat_cols = df.select_dtypes(include='object').columns\n",
    "        for cat_name in cat_cols:\n",
    "            value_counts = df[cat_name].value_counts().reset_index()\n",
    "            # Rename the columns\n",
    "            value_counts.columns = ['Class', 'Frequency']\n",
    "\n",
    "            # Print the result as a table\n",
    "            print(f\"{cat_name} frequency table\")\n",
    "            print(tabulate(value_counts, headers='keys', tablefmt='pretty'))\n",
    "\n",
    "            # Calculate relative frequency\n",
    "            total_count = value_counts['Frequency'].sum()\n",
    "            value_counts['Relative Frequency %'] = round((value_counts['Frequency'] / total_count)*100,2)\n",
    "\n",
    "            # Print the result as a table\n",
    "            print(f\"{cat_name} Relative frequency table\")\n",
    "            print(tabulate(value_counts, headers='keys', tablefmt='pretty'))\n",
    "\n",
    "            # Extract the values and index from value counts\n",
    "            value_counts = df[cat_name].value_counts()\n",
    "            values = value_counts.values\n",
    "            labels = value_counts.index\n",
    "\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # 1 row, 2 columns\n",
    "            # Create a bar graph\n",
    "            axs[0].bar(labels, values)\n",
    "            axs[0].set_title(f'Frequency of {cat_name}')\n",
    "            axs[0].set_xlabel('Categories')  # Set x-label\n",
    "            axs[0].set_ylabel('Count')       # Set y-label\n",
    "\n",
    "            axs[1].pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "            axs[1].set_title(f'Relative Frequency of {cat_name}')\n",
    "            plt.tight_layout()\n",
    "            # Show the plot\n",
    "            plt.show()  \n",
    "            \n",
    "pipeline_cat_var = Pipeline([\n",
    "    ('cat_univaraite_analysis', BarPieChartTransformer())\n",
    "])\n",
    "\n",
    "# Fit and transform your data using the pipeline\n",
    "processed_data = pipeline_cat_var.fit_transform(df[[\"Service Classification\", \"Grouped Category\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf40265",
   "metadata": {
    "cellUniqueIdByVincent": "cf11a"
   },
   "source": [
    "## 6. Preprocessing Web Service Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc2fe8",
   "metadata": {
    "cellUniqueIdByVincent": "6697b"
   },
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
    "\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "df['Processed Description'] = df['Service Description'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a22ed",
   "metadata": {
    "cellUniqueIdByVincent": "8fe81"
   },
   "source": [
    "## 7. Feature Extraction from Web Service Descriptions using TF-IDF and Sentence Transformers (all-MiniLM-L6-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c93d4",
   "metadata": {
    "cellUniqueIdByVincent": "91de5"
   },
   "outputs": [],
   "source": [
    "def vectorize_descriptions(service_list, max_features=1000):\n",
    "    \"\"\"\n",
    "    Converts a list of service descriptions into a DataFrame of TF-IDF features.\n",
    "    Limits to top `max_features` terms by importance across the corpus.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(service_list)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "\n",
    "def embed_descriptions_with_sbert(services, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Converts service descriptions into SBERT embeddings using SentenceTransformer.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(services, show_progress_bar=True)\n",
    "    \n",
    "    return pd.DataFrame(embeddings, index=services.index if isinstance(services, pd.Series) else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49964684",
   "metadata": {
    "cellUniqueIdByVincent": "9c192"
   },
   "outputs": [],
   "source": [
    "n=50\n",
    "\n",
    "tfidf_df = vectorize_descriptions(df['Processed Description'])\n",
    "tfidf_output_csv = f\"Processed_Top_{n}_Web_Services_TFIDF.csv\"\n",
    "tfidf_df.to_csv(tfidf_output_csv, encoding='utf-8', index=False, header=True)\n",
    "print(f\"TF-IDF features saved to: {tfidf_output_csv}\")\n",
    "\n",
    "embedding_df = embed_descriptions_with_sbert(df['Processed Description'])\n",
    "embedding_output_csv = f\"Processed_Top_{n}_Web_Services_SBERT_Embeddings.csv\"\n",
    "embedding_df.to_csv(embedding_output_csv, encoding='utf-8', index=False, header=True)\n",
    "print(f\"SBERT embeddings saved to: {embedding_output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151457f",
   "metadata": {
    "cellUniqueIdByVincent": "44d93"
   },
   "source": [
    "## 8. Models\n",
    "\n",
    "### 8.1 TF-IDF + SBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473bbc45",
   "metadata": {
    "cellUniqueIdByVincent": "c21db"
   },
   "source": [
    "## 9. Web Service Classification using LogReg, Random Forest, XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125d8c6",
   "metadata": {
    "cellUniqueIdByVincent": "c1bfe"
   },
   "outputs": [],
   "source": [
    "ml_dl_bert_model_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417b80b",
   "metadata": {
    "cellUniqueIdByVincent": "db7cf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_and_report_logistic(X_train, X_test, y_train, y_test, name=\"Model\"):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n{name} Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "    print(\"\\nConfusion Matrix:\\n\")\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=np.unique(y_test), \n",
    "                yticklabels=np.unique(y_test))\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"charts/ml_confusion_matrix_{name.replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        \"model\": f\"LogReg\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "def train_and_report_random_forest(X_train, X_test, y_train, y_test, name=\"Model\"):\n",
    "    # Compute class weights\n",
    "    classes = np.unique(y_train)\n",
    "    weights = class_weight.compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weights_dict = dict(zip(classes, weights))\n",
    "\n",
    "    # Train model with class weights\n",
    "    model = RandomForestClassifier(n_estimators=200, class_weight=class_weights_dict, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f\"\\n {name} Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "    print(\"\\nConfusion Matrix:\\n\")\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=np.unique(y_test), \n",
    "                yticklabels=np.unique(y_test))\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"charts/ml_confusion_matrix_{name.replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        \"model\": f\"RF\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "def train_and_report_xgboost(X_train, X_test, y_train_text, y_test_text, name=\"Model\"):\n",
    "    # Encode class labels\n",
    "    y_train = label_encoder.fit_transform(y_train_text)\n",
    "    y_test = label_encoder.transform(y_test_text)\n",
    "\n",
    "    # Compute class weights\n",
    "    classes = np.unique(y_train)\n",
    "    weights = class_weight.compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weights_dict = dict(zip(classes, weights))\n",
    "    sample_weights = np.array([class_weights_dict[label] for label in y_train])\n",
    "\n",
    "    # Train model\n",
    "    model = XGBClassifier(n_estimators=200, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Decode labels for reporting\n",
    "    y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "    y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "    print(f\"\\n{name} Classification Report:\\n\")\n",
    "    print(classification_report(y_test_labels, y_pred_labels))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "    print(\"\\nConfusion Matrix:\\n\")\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=np.unique(y_test), \n",
    "                yticklabels=np.unique(y_test))\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"charts/ml_confusion_matrix_{name.replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        \"model\": f\"XGB\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "for n in [50]:\n",
    "\n",
    "    tfidf_df = pd.read_csv(f\"Processed_Top_{n}_Web_Services_TFIDF.csv\")\n",
    "    sbert_df = pd.read_csv(f\"Processed_Top_{n}_Web_Services_SBERT_Embeddings.csv\")\n",
    "    original_df = pd.read_csv(f\"../data/Balanced_Top_{n}_Web_Services.csv\")\n",
    "    labels = original_df['Grouped Category'].fillna(\"Unknown\")\n",
    "\n",
    "    combined_df = pd.concat([tfidf_df, sbert_df], axis=1)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    result=train_and_report_logistic(X_train, X_test, y_train, y_test, name=f\"LogReg TFIDF + SBERT with {n} Web Services\")\n",
    "    ml_dl_bert_model_results.append(result)\n",
    "    \n",
    "    result=train_and_report_random_forest(X_train, X_test, y_train, y_test, name=f\"RF TFIDF + SBERT with {n} Web Services\")\n",
    "    ml_dl_bert_model_results.append(result)\n",
    "    \n",
    "    result=train_and_report_xgboost(X_train, X_test, y_train, y_test, name=f\"XGB TFIDF + SBERT with {n} Web Services\")\n",
    "    ml_dl_bert_model_results.append(result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4ac86",
   "metadata": {},
   "source": [
    "## 10.  Web Service Classification using DL  Model:  BI-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    return text\n",
    "\n",
    "\n",
    "def build_dl_model(input_length, num_classes, model_type=\"rnn\"):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=input_length))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_dl_model(df, model_type, name, max_len=150):\n",
    "    df = df.dropna(subset=[\"Service Description\", \"Grouped Category\"])\n",
    "    df[\"Service Description\"] = df[\"Service Description\"].apply(clean_text)\n",
    "\n",
    "    texts = df['Service Description'].values\n",
    "    labels = df['Grouped Category'].values\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    X_seq = tokenizer.texts_to_sequences(texts)\n",
    "    X_pad = pad_sequences(X_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(labels)\n",
    "    y_cat = to_categorical(y_enc)\n",
    "    num_classes = y_cat.shape[1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pad, y_cat, test_size=0.2, random_state=42, stratify=y_cat)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_enc), y=y_enc)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    model = build_dl_model(input_length=max_len, num_classes=num_classes, model_type=model_type)\n",
    "\n",
    "    # Train and capture history\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Plotting training history\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    class_names = le.inverse_transform(np.arange(num_classes))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"charts/dl_confusion_matrix_{name.replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=50\n",
    "df = pd.read_csv(f\"../data/Balanced_Top_{n}_Web_Services.csv\")\n",
    "#labels = original_df['Grouped Category'].fillna(\"Unknown\")\n",
    "name=\"BI-LSTM\"\n",
    "result = train_dl_model(df, model_type, name)\n",
    "ml_dl_bert_model_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10256ab",
   "metadata": {
    "cellUniqueIdByVincent": "00078"
   },
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9052b4",
   "metadata": {
    "cellUniqueIdByVincent": "15e01"
   },
   "outputs": [],
   "source": [
    "print(ml_dl_bert_model_results)\n",
    "# Save all results\n",
    "with open(\"results/ml_dl_bert_model_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ml_dl_bert_model_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e0f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(ml_dl_bert_model_results)\n",
    "\n",
    "# Set plot style\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "bar_width = 0.2\n",
    "x = range(len(df))\n",
    "\n",
    "# Plot each metric\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.bar([p + bar_width*i for p in x], df[metric], width=bar_width, label=metric.capitalize())\n",
    "\n",
    "# Labeling\n",
    "plt.xticks([p + bar_width for p in x], df['model'])\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.7, 0.8)\n",
    "plt.title('Model Comparison on 50 Web Services')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vincent": {
   "sessionId": "cda9d7de534dbd5c642e5ea2_2025-06-10T11-45-52-176Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
